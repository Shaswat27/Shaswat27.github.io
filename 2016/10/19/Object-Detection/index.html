<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0" />

    <!--Description-->
    
        <meta name="description" content="Personal blog and porfolio page">
    

    <!--Author-->
    
        <meta name="author" content="Shaswat Datta">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Object Detection"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Shaswat Datta"/>

    <!--Page Cover-->
    
        <meta property="og:image" content="undefined"/>
    

    <!-- Title -->
    
    <title>Object Detection - Shaswat Datta</title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/reset.css">
    <link rel="stylesheet" href="/css/main.css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-83741901-1', 'auto');
        ga('send', 'pageview');

    </script>



</head>

<body>

<!-- Menu -->
<!-- Navigation -->
<header>
    <div class="logo">
        <a href="/">Shaswat Datta</a>
    </div><!-- end logo -->

    <div id="menu_icon"></div>
    <nav>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives">Archives</a>
            </li>
            
            <li>
                <a href="/about">About</a>
            </li>
            
        </ul>
    </nav><!-- end navigation menu -->

    <div class="footer clearfix">
        <ul class="social clearfix">
            
            
                <li><a href="https://www.facebook.com/shaswat27" class="fb" target="_blank" data-title="Facebook"></a></li>
            
            
            
                <li><a href="https://in.linkedin.com/in/shaswat-datta-86812075" class="google" target="_blank" data-title="LinkedIn"></a></li>
            
            
                <li><a href="mailto:shaswat.datta@gmail.com" class="dribble" target="_blank" data-title="Email"></a></li>
            
            
                <li><a href="https://www.github.com/shaswat27" class="rss" target="_blank" data-title="Github"></a></li>
            
        </ul><!-- end social -->

        <div class="rights">
            <p></p>
            <p></p>
            <p>Copyright © Shaswat Datta.</p>
        </div><!-- end rights -->
    </div ><!-- end footer -->
</header><!-- end header -->


<!-- Main Content -->
<section class="main clearfix">

    <section class="top" style="background: url('/images/obs-det.jpg');">
        <div class="wrapper content_header clearfix">
            

<div class="work_nav">

    <ul class="btn clearfix">
        
        <li><a class="previous disabled"></a></li>
        
        <li><a href="/" class="grid" data-title="Portfolio"></a></li>
        
        <li><a class="next disabled"></a></li>
        
    </ul>

</div><!-- end work_nav -->
            <h1 class="title">Object Detection</h1>
        </div>
    </section><!-- end top -->

    <section class="wrapper">
        <div class="content">

            <!-- Gallery -->
            

            <!-- Content -->
            <p>One of the major tasks which need to be accomplished for the purpose of making driverless cars a reality is the ability to detect objects in an image. In essence, the car must be able to <strong><em>see</em></strong> what is front of it. I’ll digress a bit here. When I say ‘<strong><em>see</em></strong>‘, I mean the act of perception: the intermingling of the visual stimulus with our acquired memory. This is essentially the process which allows you to recognize a Coke can in a far-away shelf. This act of <em>perceiving</em> can be broken down into two separate problems:</p>
<ol>
<li><strong>Detecting where the object lies in an image: creating a bounding box</strong><br>This is also referred to as generation of region proposals and historically this has been done by various search strategies such as the Selective Search strategy.</li>
<li><strong>Identification of the object within the bounding box</strong><br>This problem is just the image classification problem, i.e. given an image  (which is just as big as the object), the computer must identify the image. This has been tackled to a great extent by using Convolutional Neural Networks (CNN) as I have mentioned in my post regarding MNIST digits classification.</li>
</ol>
<p><img src="/images/obs-det4.png" alt="the network"></p>
<p>The efficient solution, however, lies in the coupling of these two strategies as proposed by the brilliant <a href="https://arxiv.org/pdf/1506.01497v3.pdf" target="_blank" rel="external">paper</a> by Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Ren and others came up with the <strong>faster R-CNN</strong> network which tackles both the above-mentioned problems simultaneously and produces the bounding boxes for the detecting objects and classifies them as well. The network itself consists of 2 subnetworks - a region proposal network (RPN) and a standard classification network. Thus, the entire network has four loss functions - <em>classification loss</em> and <em>bounding-box regression loss</em> for each of the two subnetworks; the intuition being that the classification and detection problems influence each other. Phrased simply - the location of the bounding box depends on whether an object is in it or not, which in turn depends on the accuracy of the classification network. </p>
<p>Now, coming back to the task at hand - detecting objects in an image for driverless cars. Me, Shubh Agarwal (a batchmate) and Vikram Mohanty (a final year student) took up this task. We decided the major objects needed to be identified were - traffic lights, traffic signs, pedestrians and other vehicles. With the knowledge that the new faster R-CNN network takes about 0.2s per image at test time (on a NVIDIA GTX-980 graphic card), it was a natural choice for the task.</p>
<h3 id="Setting-Up"><a href="#Setting-Up" class="headerlink" title="Setting Up"></a>Setting Up</h3><p>The first job was to run the faster R-CNN network for the PASCAL-VOC dataset. This was the standard code uploaded on <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">GitHub</a>. As one can see, this has been written in python using Caffe wrappers. Setting up the codebase on the workstation was a fairly simple task, and on the GPU the smaller VGG-1024M network took 0.079s per image (on an average) while the deeper VGG16 network took 0.2s (as mentioned in the paper). So, we were sure that this could be used for real-time applications in driverless cars.</p>
<h3 id="Formatting-the-Custom-Dataset"><a href="#Formatting-the-Custom-Dataset" class="headerlink" title="Formatting the Custom Dataset"></a>Formatting the Custom Dataset</h3><p>Due to the specifications in Caffe, the dataset has to be cleaned and formatted to create the LMDB database. A good tutorial for the same has been posted by <a href="https://github.com/deboc/py-faster-rcnn/blob/master/help/Readme.md" target="_blank" rel="external">deboc</a>. In addition to these steps we also had to parse the ground truth annotations to filter out erroneous data in the custom dataset (e.g. bounding boxes greater than the image size, negative image coordinates). Finally, after two nights of rigorous work, encountering tons of errors during training, tracing them back in the code, the dataset was ready. In the process I also learned, the countless changes to be made to adapt the data to the <em>implicit</em> specifications of the faster R-CNN network. Most of these issues are highlighted thoroughly in the GitHub issue <a href="https://github.com/rbgirshick/py-faster-rcnn/issues/27" target="_blank" rel="external">thread</a>.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>After days (more precisely, nights) of hard work, the training finally started. “Alas!”, we thought. But success was elusive. The training stopped after just one iteration of the optimizer with no errors. Turns out this is very common <a href="https://github.com/rbgirshick/py-faster-rcnn/issues/9" target="_blank" rel="external">issue</a>, which happens due to the presence of empty bounding box data. We were forced to go back to the dataset and weed out the problem. Cleaning it, we figured out the problem - a lone image in a dataset of thousands with annotated ground truth. This resolved the error, but that didn’t solve the original issue. Skimming through the code, the problem was evident. PASCAL-VOC was a 1-indexed dataset, while ours was a 0-indexed one. With the necessary changes, we ran the training again, and this time, it worked! The training took all night.<br>As is turns out the training of the network follows an <strong>alternating optimization</strong> scheme and not an end-to-end one as followed in most neural net architectures. This so due to the non-differentiability of the ROI (region of interest) Pooling layer in that scenario. The overview of this alternating scheme is as follows (let M0 be an ImageNet pre-trained network):</p>
<ol>
<li>Train the RPN initialized with weights of M0 to get the weights M1</li>
<li>Generate the region proposals based on M1. Let the proposals be denoted by P1. </li>
<li>Train the classification network initialized with weights M0 and region proposals P1. Let these weights be M2.</li>
<li>Train the RPN initialized with weights M2 without changing the classification network to get weights M3.</li>
<li>Generate the region proposals based on M3. Let the proposals be denoted by P2. </li>
<li>Train the classification network initialized with weights M3 and region proposals P2. Let these weights be M4.</li>
<li>Return the network weights with RPN weights as M3 and classification weights as M4.</li>
</ol>
<p>More details of the same and the explanation of the <a href="http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf" target="_blank" rel="external">non-differentiability</a> of the ROI Pool layer is explained in the ICCV’15 tutorial titled ‘Training R-CNNs of various velocities’ by Ross Girshick. </p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>A few of the test results:<br><img src="/images/obs-det3.png" alt="result1"><br><img src="/images/obs-det2.png" alt="result2"></p>
<h3 id="Further-Work"><a href="#Further-Work" class="headerlink" title="Further Work"></a>Further Work</h3><p>So far, we have trained the network for the imagenet classes and vehicles like cars, buses, and airplanes only. The dataset for traffic lights, traffic signs, and pedestrians will be added soon.</p>
<h3 id="Tackling-Images-of-Various-Sizes"><a href="#Tackling-Images-of-Various-Sizes" class="headerlink" title="Tackling Images of Various Sizes"></a>Tackling Images of Various Sizes</h3><p>An important problem also solved by this network is the problem of handling images of various sizes. In real life, we rarely come across a dataset with equal image sizes. To tackle these problem the <strong>ROI Pool layer</strong> (ROIs can be different sizes) takes as input the various ROIs with variable sizes and produces a fixed size output defined by the ‘pool_w’ and ‘pool_h’ (for width and height of the output respectively, the number of channels remains the same) parameters in the layer:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: <span class="string">"roi_pool1"</span></div><div class="line">  <span class="built_in">type</span>: <span class="string">"ROIPooling"</span></div><div class="line">  bottom: <span class="string">"pool1"</span></div><div class="line">  bottom: <span class="string">"rois"</span></div><div class="line">  top: <span class="string">"pool2"</span></div><div class="line">  roi_pooling_param &#123;</div><div class="line">    pooled_w: 10</div><div class="line">    pooled_h: 10</div><div class="line">    spatial_scale: 0.0625 <span class="comment"># 1/16</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>The technique used by this layer is Spatial Pyramid Pooling (SPP) as described in the <a href="https://arxiv.org/pdf/1406.4729v4.pdf" target="_blank" rel="external">paper</a> by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. </p>


            <!-- Tags -->
            


<div class="tags">
    
</div>



            <!-- Comments -->
            <div>
                


            </div>
        </div><!-- end content -->
    </section>
</section><!-- end main -->

<!-- After footer scripts -->

<!-- jQuery -->
<script src="/js/jquery.js"></script>

<!-- Custom Code -->
<script src="/js/main.js"></script>

<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->


</body>

</html>